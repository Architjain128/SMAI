{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c54141",
   "metadata": {},
   "source": [
    "## Spam Email Classifier with KNN using TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17102e",
   "metadata": {},
   "source": [
    "1.   Assignment must be implemented in Python 3 only.\n",
    "2.   You are allowed to use libraries for data preprocessing (numpy, pandas, nltk etc) and for evaluation metrics, data visualization (matplotlib etc.).\n",
    "3.   You will be evaluated not just on the overall performance of the model and also on the experimentation with hyper parameters, data prepossessing techniques etc.\n",
    "4.   The report file must be a well documented jupyter notebook, explaining the experiments you have performed, evaluation metrics and corresponding code. The code must run and be able to reproduce the accuracies, figures/graphs etc.\n",
    "5.   For all the questions, you must create a train-validation data split and test the hyperparameter tuning on the validation set. Your jupyter notebook must reflect the same.\n",
    "6.   Strict plagiarism checking will be done. An F will be awarded for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34a310",
   "metadata": {},
   "source": [
    "**Task: Given an email, classify it as spam or ham**\n",
    "\n",
    "Given input text file (\"emails.txt\") containing 5572 email messages, with each row having its corresponding label (spam/ham) attached to it.\n",
    "\n",
    "This task also requires basic pre-processing of text (like removing stopwords, stemming/lemmatizing, replacing email_address with 'email-tag', etc..).\n",
    "\n",
    "You are required to find the tf-idf scores for the given data and use them to perform KNN using Cosine Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f091428",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c87696",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5a1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/archit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/archit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import tabulate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk.corpus\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "lemmer=nltk.WordNetLemmatizer()\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "from sklearn.metrics import r2_score,confusion_matrix,recall_score,precision_score,f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef4dff",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f178f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./emails.txt\"\n",
    "data = pd.read_csv(path, sep='\\t', header=None)\n",
    "data.rename({0: 'label', 1:'mail_text'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ef5ba",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1733d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_up(text):\n",
    "    '''\n",
    "        convert all uppercase letters to lowercase\n",
    "        replace email address with 'email-tag'\n",
    "        stop words removal\n",
    "        lemming the words\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    if text in stop_words or len(text)<3 or text in string.punctuation:\n",
    "        return False, text\n",
    "    regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    if(re.fullmatch(regex, text)):\n",
    "        return True, 'email-tag'\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»]))\"\n",
    "    if(re.fullmatch(url_regex, text)):\n",
    "        return True, 'url-tag'\n",
    "    if text.isalnum():\n",
    "        return True, lemmer.lemmatize(text)\n",
    "    return False, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a55bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_pun(sentence):\n",
    "    token_list=[]\n",
    "    sentence = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", sentence)\n",
    "    for i in sentence.split():\n",
    "        a,b=del_up(i)\n",
    "        if a==True:\n",
    "            token_list.append(b)\n",
    "    return token_list\n",
    "\n",
    "def label_it(label):\n",
    "    if label == 'ham':\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cf1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_text'] = data['mail_text'].apply(lambda x: tok_pun(x))\n",
    "data[\"label_val\"] = data[\"label\"].apply(lambda x: label_it(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebf4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words_in_data(data):\n",
    "    data.head()\n",
    "    columns=[]\n",
    "    for x in data['tokenized_text']:\n",
    "        for y in x:\n",
    "            if y not in columns:\n",
    "                columns.append(y)\n",
    "    return columns\n",
    "\n",
    "def idf(data,unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(data)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for x in data['tokenized_text']:\n",
    "            if i in x:\n",
    "                count=count+1\n",
    "        idf_dict[i]=(math.log((N)/(count)))\n",
    "    return idf_dict \n",
    "\n",
    "def tfidf(data,idf_dic):\n",
    "    tfidf_array=[]\n",
    "    temp_arr={}\n",
    "    for i in range(len(data)):\n",
    "        x=data\n",
    "        temp_arr={}\n",
    "        if len(x)==0:\n",
    "            print(i)\n",
    "            pass\n",
    "        else:\n",
    "            for y in x:\n",
    "                if y not in temp_arr:\n",
    "                    temp_arr[y]=0\n",
    "                temp_arr[y]+=1\n",
    "            \n",
    "            for y in temp_arr:\n",
    "                temp_arr[y]=math.log(1+temp_arr[y])*idf_dic[y]\n",
    "    return temp_arr\n",
    "    return tfidf_array\n",
    "\n",
    "def tfidf2(data,idf_dic):\n",
    "    tfidf_array=[]\n",
    "    temp_arr={}\n",
    "    for i in range(len(data)):\n",
    "        x=data.loc[i][\"tokenized_text\"]\n",
    "        temp_arr={}\n",
    "        if len(x)==0:\n",
    "            pass\n",
    "            # print(i)\n",
    "        else:\n",
    "            for y in x:\n",
    "                if y not in temp_arr:\n",
    "                    temp_arr[y]=0\n",
    "                temp_arr[y]+=1\n",
    "            \n",
    "            for y in temp_arr:\n",
    "                temp_arr[y]=math.log(1+temp_arr[y])*idf_dic[y]\n",
    "        tfidf_array.append(temp_arr)\n",
    "    return tfidf_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc32adc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7452"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = unique_words_in_data(data)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca841cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dic=idf(data,unique_words)\n",
    "# tfidf_array=tfidf(data,idf_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcfd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tfidf\"] = data[\"tokenized_text\"].apply(lambda x: tfidf(x,idf_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a92db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[]\n",
    "for i in range(len(data)):\n",
    "    if len(data.loc[i][\"tokenized_text\"])==0:\n",
    "        arr.append(i)\n",
    "data = data.drop(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76767a7",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75e6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data[\"tfidf\"], data['label_val'], train_size=0.8,random_state=0)\n",
    "test_X, val_X, test_y, val_y = train_test_split(test_X,test_y, test_size=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6eb76b",
   "metadata": {},
   "source": [
    "### Train your KNN model (reuse previously iplemented model built from scratch) and test on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22baf6b2",
   "metadata": {},
   "source": [
    "***1. Experiment with different distance measures [Euclidean distance, Manhattan distance, Hamming Distance] and compare with the Cosine Similarity distance results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a4ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderator(dic1,dic2):\n",
    "    dic3={}\n",
    "    dic4={}\n",
    "    for x in dic1:\n",
    "        dic3[x]=dic1[x]\n",
    "        dic4[x]=0.0\n",
    "    for x in dic2:\n",
    "        dic3[x]=0.0\n",
    "        dic4[x]=dic2[x]\n",
    "    dic3=np.array(list(dic3.values()))\n",
    "    dic4=np.array(list(dic4.values()))\n",
    "    return dic3,dic4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8345c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(X,Y):\n",
    "    X,Y=moderator(X,Y)\n",
    "    return np.sqrt(np.sum((X - Y)**2))\n",
    "\n",
    "def manhattan_distance(X,Y):\n",
    "    X,Y=moderator(X,Y)\n",
    "    return np.sum(np.abs(X - Y))\n",
    "\n",
    "def hamming_distance(X,Y):\n",
    "    X,Y=moderator(X,Y)\n",
    "    return np.sum(X != Y)\n",
    "\n",
    "def cosine_similarity_distance(X,Y):\n",
    "    X,Y=moderator(X,Y)\n",
    "    val=0.0\n",
    "    a=0.0\n",
    "    b=0.0\n",
    "    for idx in range(len(X)):\n",
    "        val=val+(X[idx]*Y[idx])\n",
    "        a=a+(X[idx]*X[idx])\n",
    "        b=b+(Y[idx]*Y[idx])\n",
    "    if(a*b==0):\n",
    "        return 0.0\n",
    "    return 1.0 - (val / (math.sqrt(a*b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f1bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(email_X,train_X,train_y,k_value,distance_metric,half=False):\n",
    "    arr1=email_X\n",
    "    arr2=[]\n",
    "    scores={}\n",
    "    for idx in train_X.keys():\n",
    "        arr2=train_X[idx]\n",
    "        scores[idx]=distance_metric(arr1,arr2)\n",
    "    sorted_scores=sorted(scores.items(), key=lambda x:x[1], reverse=False)\n",
    "    if half==True:\n",
    "        return sorted_scores\n",
    "    \n",
    "    sorted_scores=sorted_scores[:k_value]\n",
    "    spam_label=[]\n",
    "    ham_label=[]\n",
    "    cur_k=0\n",
    "    for x in sorted_scores:\n",
    "        if cur_k==k_value:\n",
    "            break\n",
    "        val=x[1]\n",
    "        x=x[0]\n",
    "        if train_y[x]==0:\n",
    "            ham_label.append(val)\n",
    "        else:\n",
    "            spam_label.append(val)\n",
    "        cur_k+=1\n",
    "    if len(ham_label)>len(spam_label):\n",
    "        return 0\n",
    "    elif len(ham_label)<len(spam_label):\n",
    "        return 1\n",
    "    else:\n",
    "        if ham_label[0]<spam_label[0]:\n",
    "            return 1\n",
    "        elif ham_label[0]>spam_label[0]:\n",
    "            return 0\n",
    "        else:\n",
    "            return random.choice([0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3101e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddfast(sorted_scores,k_max):\n",
    "    arr=[]\n",
    "    cur_k=0\n",
    "    for x in sorted_scores:\n",
    "        if cur_k==k_max:\n",
    "            break\n",
    "        val=x[1]\n",
    "        x=x[0]\n",
    "        if train_y[x]==0:\n",
    "            arr.append(0)\n",
    "        else:\n",
    "            arr.append(1)\n",
    "        cur_k+=1\n",
    "    for i in range(1,len(arr)):\n",
    "        arr[i]=(arr[i-1]+arr[i])\n",
    "    for i in range(len(arr)):\n",
    "        if(arr[i]/len(arr)>0.5):\n",
    "            arr[i]=1\n",
    "        else:\n",
    "            arr[i]=0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d9d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_knn_predict_fast(test_X,train_X,train_y,k_values,distance_metric=cosine_similarity_distance):\n",
    "    '''\n",
    "        distance matrix allowed are:\n",
    "            euclidean_distance,\n",
    "            hamming_distance,\n",
    "            manhattan_distance,\n",
    "            cosine_similarity_distance(default)\n",
    "    '''\n",
    "    predicted_label={}\n",
    "    for idx in test_X.keys():\n",
    "        sorted_dic=predict_label(test_X[idx],train_X,train_y,-1,distance_metric,True)\n",
    "        predicted_label[idx]=ddfast(sorted_dic,k_values[len(k_values)-1])\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcf8a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_knn_predict(test_X,train_X,train_y,k_value,distance_metric=cosine_similarity_distance,half=False):\n",
    "    '''\n",
    "        distance matrix allowed are:\n",
    "            euclidean_distance,\n",
    "            hamming_distance,\n",
    "            manhattan_distance,\n",
    "            cosine_similarity_distance(default)\n",
    "    '''\n",
    "    predicted_label={}\n",
    "    for idx in test_X.keys():\n",
    "        if(idx%100==0):\n",
    "            print(idx,end=\"\\r\")\n",
    "        predicted_label[idx]=predict_label(test_X[idx],train_X,train_y,k_value,distance_metric,half)\n",
    "    print(\"done!\")\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b67615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_knn_accuracy(predicted_label,test_y):\n",
    "    count=0\n",
    "    for x in predicted_label.keys():\n",
    "        if predicted_label[x]==test_y[x]:\n",
    "            count+=1\n",
    "    return count/len(predicted_label)\n",
    "\n",
    "def my_knn_mse(predicted_label,test_y):\n",
    "    count=0\n",
    "    for x in predicted_label.keys():\n",
    "        if predicted_label[x]!=test_y[x]:\n",
    "            count+=1\n",
    "    return count/len(predicted_label)\n",
    "\n",
    "def my_confusion_score(predicted_label,test_y):\n",
    "    y_pred=[]\n",
    "    y_true=test_y\n",
    "    for x in predicted_label.keys():\n",
    "        y_pred.append(predicted_label[x])\n",
    "    return confusion_matrix(y_true,y_pred)\n",
    "\n",
    "def my_f1_score(predicted_label,test_y):\n",
    "    y_pred=[]\n",
    "    y_true=test_y\n",
    "    for x in predicted_label.keys():\n",
    "        y_pred.append(predicted_label[x])\n",
    "    return f1_score(y_true,y_pred)\n",
    "\n",
    "def my_recall_score(predicted_label,test_y):\n",
    "    y_pred=[]\n",
    "    y_true=test_y\n",
    "    for x in predicted_label.keys():\n",
    "        y_pred.append(predicted_label[x])\n",
    "    return recall_score(y_true,y_pred)\n",
    "\n",
    "def my_precision_score(predicted_label,test_y):\n",
    "    y_pred=[]\n",
    "    y_true=test_y\n",
    "    for x in predicted_label.keys():\n",
    "        y_pred.append(predicted_label[x])\n",
    "    return precision_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c5d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric,k=False,arr=[],half=False):\n",
    "    k_values=[]\n",
    "    pre_k_acc=[]\n",
    "    pre_f1=[]\n",
    "    pre_con=[]\n",
    "    pre_re=[]\n",
    "    pre_pr=[]\n",
    "    pre_k_acc_val=[]\n",
    "    if k==True :\n",
    "        k_values=arr\n",
    "    else:\n",
    "        for i in range(1,15,2):\n",
    "            k_values.append(i)\n",
    "    if half==True:\n",
    "        tempa=my_knn_predict_fast(val_X,train_X,train_y,k_values,distance_metric)\n",
    "        for i in k_values:\n",
    "            print(i)\n",
    "            temp={}\n",
    "            for idx in tempa.keys():\n",
    "                temp[idx]=tempa[idx][i]\n",
    "            pre_k_acc_val.append(temp)\n",
    "            pre_k_acc.append(my_knn_accuracy(temp,val_y))\n",
    "            pre_f1.append(my_f1_score(temp,val_y))\n",
    "            pre_con.append(my_confusion_score(temp,val_y))\n",
    "            pre_re.append(my_recall_score(temp,val_y))\n",
    "            pre_pr.append(my_precision_score(temp,val_y))\n",
    "        return k_values,pre_k_acc_val,pre_k_acc,pre_f1,pre_con,pre_re,pre_pr\n",
    "    else:\n",
    "        for i in k_values:\n",
    "            temp=my_knn_predict(val_X,train_X,train_y,i,distance_metric,half)\n",
    "            pre_k_acc_val.append(temp)\n",
    "            pre_k_acc.append(my_knn_accuracy(temp,val_y))\n",
    "            pre_f1.append(my_f1_score(temp,val_y))\n",
    "            pre_con.append(my_confusion_score(temp,val_y))\n",
    "            pre_re.append(my_recall_score(temp,val_y))\n",
    "            pre_pr.append(my_precision_score(temp,val_y))\n",
    "        return k_values,pre_k_acc_val,pre_k_acc,pre_f1,pre_con,pre_re,pre_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6210163",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_predict=my_knn_predict_fast(test_X,train_X,train_y,[1,2,3,5,7,9,11],euclidean_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ecfd3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "17\n",
      "23\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/archit/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10845/765139407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mecu_k_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mecu_predications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mecu_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mecu_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mecu_con\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mecu_re\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mecu_pr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_diff_k_on_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistance_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meuclidean_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10845/3110717436.py\u001b[0m in \u001b[0;36mtry_diff_k_on_val\u001b[0;34m(train_X, train_y, val_X, val_y, distance_metric, k, arr, half)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtempa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mpre_k_acc_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpre_k_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_knn_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ecu_k_val,ecu_predications,ecu_acc,ecu_f1,ecu_con,ecu_re,ecu_pr=try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric=euclidean_distance,k=True,arr=[1,3,5,7,9,11,17,23,28],half=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table\n",
    "columns = ['k','Accuracy' ,'Confusion Matrix', 'Recall', 'Precision', 'F1-score']\n",
    "arr=[]\n",
    "for i in range(len(ecu_k_val)):\n",
    "    arr.append([ecu_k_val[i],ecu_acc[i],ecu_con[i],ecu_re[i],ecu_pr[i],ecu_f1[i]])\n",
    "table_data = tabulate.tabulate(arr, columns, tablefmt='fancy_grid')\n",
    "print(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7297ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecu_k_val,ecu_predications,ecu_acc,ecu_f1,ecu_con,ecu_re,ecu_pr=try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric=euclidean_distance,k=True,arr=[1,3,5,7,9,11,17,23,28],half=False)\n",
    "man_k_val,man_predications,man_acc,man_f1,man_con,man_re,man_pr=try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric=manhattan_distance,k=True,arr=[1,3,5,7,9,11,17,23,28],half=False)\n",
    "ham_k_val,ham_predications,ham_acc,ham_f1,ham_con,ham_re,ham_pr=try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric=hamming_distance,k=True,arr=[1,3,5,7,9,11,17,23,28],half=False)\n",
    "cos_k_val,cos_predications,cos_acc,cos_f1,cos_con,cos_re,cos_pr=try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric=cosine_similarity_distance,k=True,arr=[1,3,5,7,9,11,17,23,28],half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c395d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(left=1.5, bottom=1.5, right=2, top=2, wspace=1, hspace=1)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(cos_k_val,cos_acc)\n",
    "plt.title('Cosine Similarity')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(ecu_k_val,ecu_acc)\n",
    "plt.title('Euclidean Distance')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(ham_k_val,ham_acc)\n",
    "plt.title('Hamming Distance')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(man_k_val,man_acc)\n",
    "plt.title('Manhattan Distance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31dc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_predict=my_knn_predict(test_X,train_X,train_y,5,euclidean_distance)\n",
    "my_knn_accuracy(euclidean_predict,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_predict=my_knn_predict(test_X,train_X,train_y,5,manhattan_distance)\n",
    "my_knn_accuracy(manhattan_predict,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_predict=my_knn_predict(test_X,train_X,train_y,5,hamming_distance)\n",
    "my_knn_accuracy(hamming_predict,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_predict=my_knn_predict(test_X,train_X,train_y,3,cosine_similarity_distance)\n",
    "my_knn_accuracy(cosine_similarity_predict,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad1298",
   "metadata": {},
   "source": [
    "***2. Explain which distance measure works best and why? Explore the distance measures and weigh their pro and cons in different application settings.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7f84c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45a99c76",
   "metadata": {},
   "source": [
    "***3. Report ~~Mean Squared Error(MSE), Mean-Absolute-Error(MAE), R-squared (R2) score~~ F1-Score ,Confusion Matrix, Recall and Precision in a tabular form***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9668814",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_k_val,cos_predications,cos_acc,cos_f1,cos_con,cos_re,cos_pr=try_diff_k_on_val(train_X,train_y,val_X,val_y,distance_metric=cosine_similarity_distance,k=True,arr=[1,3,5,7,9,11,17,23,28],half=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table\n",
    "columns = ['k','Accuracy' ,'Confusion Matrix', 'Recall', 'Precision', 'F1-score']\n",
    "arr=[]\n",
    "for i in range(len(cos_k_val)):\n",
    "    arr.append([cos_k_val[i],cos_acc[i],cos_con[i],cos_re[i],cos_pr[i],cos_f1[i]])\n",
    "table_data = tabulate.tabulate(arr, columns, tablefmt='fancy_grid')\n",
    "print(table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dde8d3",
   "metadata": {},
   "source": [
    "***4. Choose different K values (k=1,3,5,7,11,17,23,28) and experiment. Plot a graph showing R2 score vs k.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(left=1.5, bottom=1.5, right=2, top=2, wspace=1, hspace=1)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(cos_k_val,cos_acc)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(cos_k_val,cos_f1)\n",
    "plt.title('F1-score')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(cos_k_val,cos_re)\n",
    "plt.title('Recall')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(cos_k_val,cos_pr)\n",
    "plt.title('Precision')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15000352",
   "metadata": {},
   "source": [
    "### Train and test Sklearn's KNN classifier model on your data (use metric which gave best results on your experimentation with built-from-scratch model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aab7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d22aa47",
   "metadata": {},
   "source": [
    "***Compare both the models result.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a5274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64987575",
   "metadata": {},
   "source": [
    "***What is the time complexity of training using KNN classifier?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770c106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fad1f345",
   "metadata": {},
   "source": [
    "***What is the time complexity while testing? Is KNN a linear classifier or can it learn any boundary?***"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0daaa324",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
